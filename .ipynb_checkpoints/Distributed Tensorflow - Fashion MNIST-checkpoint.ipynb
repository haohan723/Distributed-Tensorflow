{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/zalandoresearch/fashion-mnist.git\n",
    "\n",
    "\n",
    "cd fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haohanwang/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import keras\n",
    "from tensorflow.contrib import slim\n",
    "import random\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from utils import mnist_reader\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_servers = [\"pc-01:2222\"]\n",
    "workers = [ \"pc-02:2222\", \n",
    "      \"pc-03:2222\",\n",
    "      \"pc-04:2222\"]\n",
    "cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input Flags\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a server for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "server = tf.train.Server(\n",
    "    cluster,\n",
    "    job_name=FLAGS.job_name,\n",
    "    task_index=FLAGS.task_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "test_images,test_labels = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Training images shape: {shape}\".format(shape=train_images.shape))\n",
    "print(\"Training labels shape: {shape}\".format(shape=train_labels.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test images shape: {shape}\".format(shape=test_images.shape))\n",
    "print(\"Test labels shape: {shape}\".format(shape=test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a dictionary of integer labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preprocession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "x_train = train_images.reshape([-1,28,28,1])\n",
    "x_test= test_images.reshape([-1,28,28,1])\n",
    "y_train = keras.utils.to_categorical(np.ravel(train_labels), num_classes)\n",
    "y_test = keras.utils.to_categorical(np.ravel(test_labels), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build Convolutional Neural Network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "n_hidden_1 = 256 # Units in first hidden layer\n",
    "n_hidden_2 = 128 # Units in second hidden layer\n",
    "n_input = 784 # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # Fashion MNIST total classes (0-9 digits)\n",
    "image_size = 28\n",
    "channel_size = 1\n",
    "n_samples =  x_train.shape[0]\n",
    "size, num_features = train_images.shape\n",
    "batch_size = 50\n",
    "epochs = 1\n",
    "num_iterations = size//batch_size\n",
    "test_step = 500\n",
    "\n",
    "filter_size1 = 512\n",
    "filter_size2 = 128\n",
    "filter_size3 = 32\n",
    "\n",
    "sigma = 1e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "LOG_DIR = 'projector'\n",
    "\n",
    "path_to_fmnist_sprites = os.path.join(LOG_DIR,'sprites.png')\n",
    "path_to_fmnist_metadata = os.path.join(LOG_DIR,'metadata.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    '''attach a lot of summaries to a tensorboard'''\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('seddev',stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None,image_size, image_size,channel_size])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    tf.summary.image('input',x,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,w):\n",
    "    return tf.nn.conv2d(x,w, strides=[1,1,1,1],padding ='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1,2,2,1],strides = [1,2,2,1],padding ='SAME')\n",
    "\n",
    "def avgpool2d(x):\n",
    "    \n",
    "    return tf.reduce_mean(x,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    layer_flat = tf.reshape(layer,[-1,num_features])\n",
    "    #now the shape should be [num_images, img_height * img_width*num_channels]\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev = 0.1))\n",
    "\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.01, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "elif FLAGS.job_name == \"worker\":\n",
    "    # Between-graph replication\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "        cluster=cluster)):\n",
    "\n",
    "        # count the number of updates\n",
    "        global_step = tf.get_variable(\n",
    "            'global_step',\n",
    "            [],\n",
    "            initializer = tf.constant_initializer(0),\n",
    "            trainable = False)\n",
    "      \n",
    "       #Build Model Architecure\n",
    "        with tf.variable_scope('conv1'):\n",
    "            #weights shape: [height, weight, channels, number of filters]\n",
    "            w_conv1 = tf.Variable(new_weights([3,3,1,filter_size1]))\n",
    "            b_conv1 = tf.Variable(new_biases(length = filter_size1))\n",
    "            variable_summaries(w_conv1)\n",
    "            variable_summaries(b_conv1)\n",
    "\n",
    "            z1 = tf.add(conv2d(x,w_conv1),b_conv1)\n",
    "            batch_mean1, batch_var1 = tf.nn.moments(z1,[0])\n",
    "            scale1 = tf.Variable(tf.ones([filter_size1]))\n",
    "            beta1 = tf.Variable(tf.zeros([filter_size1]))\n",
    "            bn1 = tf.nn.batch_normalization(z1,batch_mean1, batch_var1, beta1, scale1, sigma)\n",
    "\n",
    "            a1 = tf.nn.relu(bn1)\n",
    "            d1 = tf.nn.dropout(a1,keep_prob = 0.8)\n",
    "        \n",
    "        with tf.variable_scope('conv2'):\n",
    "            #weights shape: [height, weight, channels, number of filters]\n",
    "            w_conv2 = tf.Variable(new_weights([3,3,512,filter_size2]))\n",
    "            b_conv2 = tf.Variable(new_biases(length = filter_size2))\n",
    "            variable_summaries(w_conv2)\n",
    "            variable_summaries(b_conv2)\n",
    "\n",
    "            z2 = tf.add(conv2d(d1,w_conv2),b_conv2)\n",
    "            batch_mean2, batch_var2 = tf.nn.moments(z2,[0])\n",
    "            scale2 = tf.Variable(tf.ones([filter_size2]))\n",
    "            beta2 = tf.Variable(tf.zeros([filter_size2]))\n",
    "            bn2 = tf.nn.batch_normalization(z2,batch_mean2, batch_var2, beta2, scale2, sigma)\n",
    "\n",
    "            a2 = tf.nn.relu(bn2)\n",
    "            p2 = avgpool2d(a2)\n",
    "            \n",
    "        # Flatten Convolution layer\n",
    "        layer_flat, num_features = flatten_layer(p2)\n",
    "        \n",
    "        \n",
    "                #Fully connected Layer\n",
    "        with tf.variable_scope('full_connected'):\n",
    "            w_fc = tf.Variable(new_weights([num_features, 10]))\n",
    "            b_fc = tf.Variable(new_biases(length = 10))\n",
    "            variable_summaries(w_fc)\n",
    "            variable_summaries(b_fc)\n",
    "\n",
    "            z4 = tf.add(tf.matmul(layer_flat,w_fc),b_fc)\n",
    "            d4 = tf.nn.dropout(z4,keep_prob = 0.8)\n",
    "\n",
    "            Y = tf.nn.softmax(d4)\n",
    "        tf.summary.histogram('fc_layer',Y)\n",
    "            \n",
    "        #cross entropy\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits = Y, labels = y)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        tf.summary.scalar('loss',cost)\n",
    "        \n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(Y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "        \n",
    "        summary_op = tf.summary.merge_all()\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        \n",
    "        ,\n",
    "    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step = global_step,init_op = init_op)\n",
    "    \n",
    "    \n",
    "    begin_time = time.time()\n",
    "    frequency = 100\n",
    "    with sv.prepare_or_wait_for_session(server.target) as sess: \n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR,'train'), graph = tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR,'test'),graph = tf.get_default_graph())\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR,graph = tf.get_default_graph())\n",
    "        \n",
    "        count = 0\n",
    "        #performe training cycles\n",
    "        start_time = time.time()\n",
    "        for i in range(num_iterations*epochs):\n",
    "            offset = (i*batch_size)% size\n",
    "            batch_x = x_train[(offset):(offset+batch_size),:]\n",
    "            batch_y = y_train[offset:(offset+batch_size),:]\n",
    "            \n",
    "            summary, _,loss,step = sess.run([merged, optimizer,cross_entropy,global_step],feed_dict={x: batch_x, y: batch_y})\n",
    "            train_writer.add_summary(summary,i)\n",
    "            \n",
    "            count += 1\n",
    "            if count % frequency == 0 or i+1 == batch_count:\n",
    "                elapsed_time = time.time() - start_time \n",
    "                start_time = time.time()\n",
    "                print('Step: %d,'%(step + 1),\n",
    "                     \"Epoch: %2d, \"%(epoch+1),\n",
    "                     'Cost: %.4f,'%loss,\n",
    "                     'AvgTime: %3.2fms'%float(elapsed_time*100/frequency))\n",
    "                \n",
    "        for i in range(test_step):\n",
    "        if i % 10 == 0:\n",
    "            summary, test_accuracy = sess.run([merged,accuracy],feed_dict={x: x_test, y: y_test})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Test accuracy at step %s: %s' % (i, test_accuracy)) \n",
    "            \n",
    "    sv.stop()\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
